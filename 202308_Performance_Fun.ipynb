{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krishnannarayanaswamy/astra-ai-demo/blob/main/202308_Performance_Fun.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing needed libraries, preping the env"
      ],
      "metadata": {
        "id": "FNybXl1UvnPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Some preparation\n",
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bg3aNb5ymaM0",
        "outputId": "0186862e-caca-43e2-b0f0-e29750ebb192"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.27.10-py3-none-any.whl (76 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/76.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.27.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask user for OpenAI API Key\n",
        "from getpass import getpass\n",
        "OPENAIKEY = getpass('Please enter your OpenAI API Key: ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jOxVv-GwD0t",
        "outputId": "18e8729f-5306-4bc2-b73b-3327fe1f98e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please enter your OpenAI API Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Optimizing the Output - FSL and COT to reduce Hallucinations"
      ],
      "metadata": {
        "id": "mSoG42On3wuQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "dZEaQ9jNl165",
        "outputId": "7507b6de-ec43-4488-98b1-925949922067"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5576c31e204e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Set up OpenAI API key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOPENAIKEY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'openai'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import openai\n",
        "import time\n",
        "\n",
        "# Set up OpenAI API key\n",
        "openai.api_key = OPENAIKEY\n",
        "\n",
        "#define initial prompt\n",
        "prompt = \"\"\"\n",
        "John has 2 houses. Each house has 3 bedrooms and there are 2 windows in each bedroom.\n",
        "Each house has 1 kitchen with 2 windows. Also, each house has 5 windows that are not in the bedrooms or kitchens.\n",
        "How many windows are there in John's houses? Explain your reasoning.\n",
        "\"\"\"\n",
        "\n",
        "# Generate final response\n",
        "response = openai.Completion.create(\n",
        "    engine=\"text-davinci-003\",\n",
        "    prompt=prompt,\n",
        "    max_tokens=100,\n",
        "    n=1,\n",
        "    stop=None,\n",
        "    temperature=0.0,\n",
        ")\n",
        "\n",
        "print(f\"Response: {response.choices[0].text.strip()}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**boah, not really true, is it?**"
      ],
      "metadata": {
        "id": "4gER5LFg5JGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's try with some samples, chain of thoughts (CoT)\n",
        "# model: text-davinci-003\n",
        "prompt = \"\"\"\n",
        "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
        "A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\n",
        "\n",
        "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
        "A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\n",
        "\n",
        "Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\n",
        "A: Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they\n",
        "had 74 - 35 = 39. The answer is 39.\n",
        "\n",
        "Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\n",
        "A: Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny 20 - 12 = 8. The answer is 8.\n",
        "\n",
        "Q: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\n",
        "A: Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. 5 + 4 = 9. The answer is 9.\n",
        "\n",
        "Q: John has 2 houses. Each house has 3 bedrooms and there are 2 windows in each bedroom.\n",
        "Each house has 1 kitchen with 2 windows. Also, each house has 5 windows that are not in the bedrooms or kitchens.\n",
        "How many windows are there in John's houses?\n",
        "A:\n",
        "\"\"\"\n",
        "\n",
        "# Generate final response\n",
        "response = openai.Completion.create(\n",
        "    engine=\"text-davinci-003\",\n",
        "    prompt=prompt,\n",
        "    max_tokens=100,\n",
        "    n=1,\n",
        "    stop=None,\n",
        "    temperature=0.9,\n",
        ")\n",
        "\n",
        "print(f\"Response: {response.choices[0].text.strip()}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltEW2IbLmY1A",
        "outputId": "2ba3ccf8-fec8-4f19-efc6-8384d615dfb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: John has 2 houses. Each house has 3 bedrooms with 2 windows each. That makes 6 windows for each house. Each house also has 1 kitchen with 2 windows and 5 windows that are not in the bedrooms or kitchens. So for each house, there are 6 + 2 + 5 = 13 windows. Since there are 2 houses, there are 13 x 2 = 26 windows in total. The answer is 26.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Summary**\n",
        "\n",
        "The chain-of-thought prompting clearly increases the ability of LLMs at certain tasks. I suggest reading the entire paper if you’d like to learn more about the experiments they conducted.\n",
        "\n",
        "It is important to note that, as also mentioned in the paper, the benefits of chain-of-thought prompting only become evident when applied to models with approximately 100 billion parameters, and it doesn’t significantly enhance the performance of smaller models.\n",
        "\n",
        "The experiment results yield to the conclusion that smaller models produce fluent but illogical chains of thought, which leads to a worse performance than standard prompting.\n",
        "\n",
        "**References**\n",
        "\n",
        "Wei, Jason, et al. (2022). Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. ArXiv. https://arxiv.org/abs/2201.11903"
      ],
      "metadata": {
        "id": "gCSPG-_hvSGT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "# 2. Discussing relevance using distance calculations\n"
      ],
      "metadata": {
        "id": "f_g3bQTfxgaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Define vector database\n",
        "vector_database = [\n",
        "    np.array([0.2, 0.3, 0.8]),\n",
        "    np.array([0.5, 0.1, 0.9]),\n",
        "    np.array([0.7, 0.5, 0.2])\n",
        "]\n",
        "\n",
        "# Define user query\n",
        "user_query = np.array([0.3, 0.6, 0.2])\n",
        "\n",
        "# Calculate cosine similarity and retrieve relevant results\n",
        "results = []\n",
        "for i, vector in enumerate(vector_database):\n",
        "    similarity = cosine_similarity([user_query], [vector])[0][0]\n",
        "    results.append((i, similarity))\n",
        "\n",
        "# Sort results by relevance\n",
        "results.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Print top k results\n",
        "k = 2\n",
        "for i in range(min(k, len(results))):\n",
        "    index, similarity = results[i]\n",
        "    print(f\"Result {i+1}: Index {index}, Similarity {similarity}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ma21HetyxsB7",
        "outputId": "ab137780-7691-4842-d7b3-b3676d629b07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result 1: Index 2, Similarity 0.889646241113611\n",
            "Result 2: Index 0, Similarity 0.651203294055074\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code, we have a vector database represented by a list of NumPy arrays. We also have a user query represented by a NumPy array. We calculate the cosine similarity between the user query and each vector in the database using the cosine_similarity function from scikit-learn.\n",
        "\n",
        "The results are stored in a list of tuples, where each tuple contains the index of the vector in the database and its similarity to the user query. We then sort the results based on similarity in descending order.\n",
        "Finally, we print the top k results, where k is the desired number of relevant results to display.\n",
        "\n",
        "This code demonstrates how cosine similarity can be used to measure the relevance of results in a vector database based on a user query. You can modify the vector database, user query, and the value of k to suit your specific needs.\n",
        "\n",
        "**step-by-step explanation of how the cosine similarity is calculated:**\n",
        "\n",
        "1. The code snippet uses the cosine_similarity function from the scikit-learn library. This function takes two arrays as input and calculates the cosine similarity between them.\n",
        "\n",
        "2. The cosine similarity is calculated by taking the dot product of the two vectors and dividing it by the product of their magnitudes. Mathematically, it can be represented as:\n",
        "Cosine Similarity Formula\n",
        "Here, A and B represent the two vectors, and ||A|| and ||B|| represent their respective magnitudes.\n",
        "\n",
        "3. The cosine similarity value ranges from -1 to 1. A value of 1 indicates that the vectors are identical, 0 indicates no similarity, and -1 indicates complete dissimilarity.\n",
        "4. In the code snippet, the cosine similarity is calculated for each vector in the vector database with the user query vector.\n",
        "The results are stored in a list of tuples, where each tuple contains the index of the vector in the database and its similarity to the user query.\n",
        "5. The results are then sorted in descending order based on similarity, so that the most similar vectors appear first.\n",
        "\n",
        "**There are several distance metrics used in vector search besides cosine similarity. Some of the commonly used distance metrics are:**\n",
        "\n",
        "1. Euclidean distance: This is the straight-line distance between two points in a multidimensional space. It is calculated as the square root of the sum of the squared differences between the corresponding components of the two vectors.\n",
        "\n",
        "2. Manhattan distance: This is the distance between two points measured along the axes at right angles. It is calculated as the sum of the absolute differences between the corresponding components of the two vectors.\n",
        "\n",
        "3. Hamming distance: This is the number of positions at which the corresponding components of two vectors are different. It is commonly used in binary data, such as DNA sequences or error-correcting codes.\n",
        "\n",
        "4. Mahalanobis distance: This is a measure of the distance between a point and a distribution. It takes into account the covariance structure of the data and is useful for data with correlated features.\n",
        "\n",
        "5. Bray-Curtis dissimilarity: This is a measure of dissimilarity between two vectors that takes into account the relative abundance of the components. It is commonly used in ecology and biology.\n",
        "\n",
        "These distance metrics can be used in vector search and recommendation systems to measure the similarity or dissimilarity between vectors. The choice of distance metric depends on the specific application and the nature of the data.\n",
        "\n",
        "Citations:\n",
        "[1] https://towardsdatascience.com/9-distance-measures-in-data-science-918109d069fa\n",
        "[2] https://weaviate.io/blog/distance-metrics-in-vector-search\n",
        "\n",
        "[3] https://medium.datadriveninvestor.com/cosine-similarity-cosine-distance-6571387f9bf8\n",
        "\n",
        "[4] https://www.learndatasci.com/glossary/cosine-similarity/\n",
        "\n",
        "[5] https://www.imaurer.com/which-vector-similarity-metric-should-i-use/\n",
        "\n",
        "[6] https://www.machinelearningplus.com/nlp/cosine-similarity/"
      ],
      "metadata": {
        "id": "8LixcnLNyFnL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**How do we have to change the above code to demonstrate Recall@K?**"
      ],
      "metadata": {
        "id": "6r8BlY0n0Zz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Define vector database\n",
        "vector_database = [\n",
        "    np.array([0.2, 0.3, 0.8]),\n",
        "    np.array([0.5, 0.1, 0.9]),\n",
        "    np.array([0.7, 0.5, 0.2]),\n",
        "    np.array([0.8, 0.4, 0.1])\n",
        "]\n",
        "\n",
        "# Define user query\n",
        "user_query = np.array([0.3, 0.6, 0.2])\n",
        "\n",
        "# Calculate cosine similarity and retrieve relevant results\n",
        "results = []\n",
        "for i, vector in enumerate(vector_database):\n",
        "    similarity = cosine_similarity([user_query], [vector])[0][0]\n",
        "    results.append((i, similarity))\n",
        "\n",
        "# Sort results by relevance\n",
        "results.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Calculate Recall@k\n",
        "k = 2\n",
        "relevant_results = [result[0] for result in results[:k]] # hint: results[:k] retrieves elements from index 0 to index k-1.\n",
        "recall = len(set(relevant_results)) / len(vector_database)\n",
        "\n",
        "print(f\"Recall@{k}: {recall}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYzsTSRty5FU",
        "outputId": "bb49dfb3-7cd3-426f-9f3b-5d9dc3487506"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recall@2: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results are stored in a list of tuples, where each tuple contains the index of the vector in the database and its similarity to the user query. We then sort the results based on similarity in descending order.\n",
        "We calculate Recall@k by considering the top k relevant results.\n",
        "\n",
        "In this example, we set k to 2. We extract the indices of the top k relevant results and calculate the recall as the ratio of the number of relevant results to the total number of vectors in the database.\n",
        "Finally, we print the Recall@k value.\n",
        "\n",
        "**Question: What happens if we add another distinct (1) vector to our database?  What is the output then? What will be the output if you increase K to 3?**"
      ],
      "metadata": {
        "id": "hZ1SZ0gq1swt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**How do we have to change the above code to demonstrate AP (Average Precision)?**"
      ],
      "metadata": {
        "id": "Hsr4zzGT1_jX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Define vector database\n",
        "vector_database = [\n",
        "    np.array([0.2, 0.3, 0.8]),\n",
        "    np.array([0.5, 0.1, 0.9]),\n",
        "    np.array([0.7, 0.5, 0.2])\n",
        "]\n",
        "\n",
        "# Define user query\n",
        "user_query = np.array([0.3, 0.6, 0.2])\n",
        "\n",
        "# Calculate cosine similarity and retrieve relevant results\n",
        "results = []\n",
        "for i, vector in enumerate(vector_database):\n",
        "    similarity = cosine_similarity([user_query], [vector])[0][0]\n",
        "    results.append((i, similarity))\n",
        "\n",
        "# Sort results by relevance\n",
        "results.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Calculate Recall@k\n",
        "k = 2\n",
        "relevant_results = [result[0] for result in results[:k]]\n",
        "recall = len(set(relevant_results)) / len(vector_database)\n",
        "\n",
        "print(f\"Recall@{k}: {recall}\")\n",
        "\n",
        "# Calculate Average Precision (AP)\n",
        "precision = []\n",
        "for i, result in enumerate(results):\n",
        "    if result[0] in relevant_results:\n",
        "        precision.append(len(set(relevant_results[:k]).intersection(set([r[0] for r in results[:i+1]]))) / (i+1))\n",
        "\n",
        "ap = sum(precision) / len(relevant_results)\n",
        "\n",
        "print(f\"AP: {ap}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQfaQboL1oTR",
        "outputId": "5e9b06b0-d8e8-42fe-942a-c8cc6da40843"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recall@2: 0.6666666666666666\n",
            "AP: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this updated code, after calculating Recall@k, we compute the precision values at each relevant result position and store them in the precision list.\n",
        "\n",
        "We then calculate the AP by taking the average of the precision values.\n",
        "Please note that the code assumes that the relevance of results is binary (relevant or not relevant).\n",
        "\n",
        "If you have a different relevance scoring system, you may need to modify the code accordingly."
      ],
      "metadata": {
        "id": "osz_Q0KA2Jmd"
      }
    }
  ]
}